name: MLflow Experiment Tracking

on:
  workflow_run:
    workflows: ["CI Pipeline"]
    branches: [master, main]
    types:
      - completed
  workflow_dispatch:
    inputs:
      experiment_name:
        description: 'MLflow experiment name'
        required: false
        default: 'weather_mean_temp_models'
      register_model:
        description: 'Register model to MLflow Model Registry'
        required: false
        default: 'false'
        type: boolean

concurrency:
  group: mlflow-${{ github.ref }}
  cancel-in-progress: true

env:
  EXPERIMENT_NAME: ${{ github.event.inputs.experiment_name || 'weather_mean_temp_models' }}
  MIN_R2_THRESHOLD: 0.90
  MAX_RMSE_THRESHOLD: 1.5

jobs:
  train-and-evaluate:
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
    outputs:
      run_id: ${{ steps.train.outputs.run_id }}
      rmse_test: ${{ steps.metrics.outputs.rmse_test }}
      r2_test: ${{ steps.metrics.outputs.r2_test }}
      model_passed: ${{ steps.validate.outputs.passed }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Cache MLflow runs
        uses: actions/cache@v4
        with:
          path: mlruns/
          key: mlflow-${{ github.ref }}-${{ github.sha }}
          restore-keys: |
            mlflow-${{ github.ref }}-
            mlflow-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install dvc

      - name: Setup MLflow tracking
        run: |
          echo "MLFLOW_TRACKING_URI=./mlruns" >> $GITHUB_ENV
          echo "MLFLOW_EXPERIMENT_NAME=${{ env.EXPERIMENT_NAME }}" >> $GITHUB_ENV

      - name: Setup DVC with DagsHub
        run: |
          dvc remote modify dagshub --local password ${{ secrets.DAGSHUB_TOKEN }}

      - name: Pull data and run pipeline
        run: |
          echo "Pulling data from DagsHub..."
          dvc pull
          echo "Running DVC pipeline..."
          dvc repro
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Run training
        id: train
        run: |
          echo "Starting training with experiment: ${{ env.EXPERIMENT_NAME }}"
          python src/models/train_model.py 2>&1 | tee training.log

          # Extract run ID if available
          if [ -f "model_info.json" ]; then
            RUN_ID=$(python -c "import json; print(json.load(open('model_info.json')).get('mlflow_run_id', 'N/A'))" 2>/dev/null || echo "N/A")
            echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
          fi
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Run evaluation
        run: |
          python src/models/evaluate_model.py --model-path models/random_forest/Production/model.pkl
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Extract metrics
        id: metrics
        run: |
          python << 'EOF'
          import json
          import os

          with open('metrics.json') as f:
              metrics = json.load(f)

          test = metrics.get('test', {})
          valid = metrics.get('valid', {})
          train = metrics.get('train', {})

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"rmse_test={test.get('rmse', 'N/A')}\n")
              f.write(f"r2_test={test.get('r2', 'N/A')}\n")
              f.write(f"mae_test={test.get('mae', 'N/A')}\n")
              f.write(f"rmse_valid={valid.get('rmse', 'N/A')}\n")
              f.write(f"r2_valid={valid.get('r2', 'N/A')}\n")
              f.write(f"rmse_train={train.get('rmse', 'N/A')}\n")
              f.write(f"r2_train={train.get('r2', 'N/A')}\n")

          print("=== Model Metrics ===")
          print(json.dumps(metrics, indent=2))
          EOF

      - name: Validate model performance
        id: validate
        run: |
          PASSED=true

          echo "Validating model against thresholds..."
          echo "  Min R² threshold: $MIN_R2_THRESHOLD"
          echo "  Max RMSE threshold: $MAX_RMSE_THRESHOLD"
          echo ""

          # Check R² score
          R2="${{ steps.metrics.outputs.r2_test }}"
          if [ "$R2" != "N/A" ]; then
            result=$(python -c "print('pass' if float('$R2') >= float('$MIN_R2_THRESHOLD') else 'fail')")
            if [ "$result" = "fail" ]; then
              echo "❌ R² ($R2) is below threshold ($MIN_R2_THRESHOLD)"
              PASSED=false
            else
              echo "✓ R² ($R2) meets threshold"
            fi
          fi

          # Check RMSE
          RMSE="${{ steps.metrics.outputs.rmse_test }}"
          if [ "$RMSE" != "N/A" ]; then
            result=$(python -c "print('pass' if float('$RMSE') <= float('$MAX_RMSE_THRESHOLD') else 'fail')")
            if [ "$result" = "fail" ]; then
              echo "❌ RMSE ($RMSE) exceeds threshold ($MAX_RMSE_THRESHOLD)"
              PASSED=false
            else
              echo "✓ RMSE ($RMSE) meets threshold"
            fi
          fi

          echo ""
          if [ "$PASSED" = "true" ]; then
            echo "✓ Model validation PASSED"
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "❌ Model validation FAILED"
            echo "passed=false" >> $GITHUB_OUTPUT
          fi

      - name: Compare with previous model
        run: |
          echo "=== Model Comparison ==="
          if git show HEAD~1:metrics.json > /tmp/prev_metrics.json 2>/dev/null; then
            python << 'EOF'
          import json

          with open('metrics.json') as f:
              current = json.load(f)
          with open('/tmp/prev_metrics.json') as f:
              previous = json.load(f)

          print("Metric         | Previous | Current  | Change")
          print("---------------|----------|----------|--------")

          for split in ['train', 'valid', 'test']:
              curr_rmse = current.get(split, {}).get('rmse', 0)
              prev_rmse = previous.get(split, {}).get('rmse', 0)
              if prev_rmse > 0:
                  change = ((curr_rmse - prev_rmse) / prev_rmse) * 100
                  symbol = "↑" if change > 0 else "↓" if change < 0 else "→"
                  print(f"RMSE ({split:5s})  | {prev_rmse:8.4f} | {curr_rmse:8.4f} | {symbol} {abs(change):5.1f}%")
          EOF
          else
            echo "No previous metrics to compare"
          fi
        continue-on-error: true

      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-artifacts
          path: |
            models/
            metrics.json
            model_info.json
            mlruns/
          retention-days: 30

      - name: Upload training logs
        uses: actions/upload-artifact@v4
        with:
          name: training-logs
          path: training.log
          retention-days: 7
        if: always()

  register-model:
    runs-on: ubuntu-latest
    needs: train-and-evaluate
    if: |
      github.event_name == 'push' &&
      (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main') &&
      needs.train-and-evaluate.outputs.model_passed == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download model artifacts
        uses: actions/download-artifact@v4
        with:
          name: model-artifacts
          path: ./

      - name: Model registration info
        run: |
          echo "========================================="
          echo "Model Registration"
          echo "========================================="
          echo "Model passed validation and is ready for registration"
          echo ""
          echo "Metrics:"
          echo "  - RMSE (test): ${{ needs.train-and-evaluate.outputs.rmse_test }}"
          echo "  - R² (test): ${{ needs.train-and-evaluate.outputs.r2_test }}"
          echo ""
          echo "To register this model to MLflow Model Registry:"
          echo "  mlflow models register -m models/random_forest/Production -n weather_model"

  create-summary:
    runs-on: ubuntu-latest
    needs: train-and-evaluate
    if: always()

    steps:
      - name: Create job summary
        run: |
          echo "## MLflow Training Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Experiment:** ${{ env.EXPERIMENT_NAME }}" >> $GITHUB_STEP_SUMMARY
          echo "**Run ID:** ${{ needs.train-and-evaluate.outputs.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Model Metrics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Split | RMSE | R² |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|------|-----|" >> $GITHUB_STEP_SUMMARY
          echo "| Test | ${{ needs.train-and-evaluate.outputs.rmse_test }} | ${{ needs.train-and-evaluate.outputs.r2_test }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.train-and-evaluate.outputs.model_passed }}" == "true" ]; then
            echo "### ✓ Model Validation Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ❌ Model Validation Failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Model did not meet performance thresholds:" >> $GITHUB_STEP_SUMMARY
            echo "- Min R²: ${{ env.MIN_R2_THRESHOLD }}" >> $GITHUB_STEP_SUMMARY
            echo "- Max RMSE: ${{ env.MAX_RMSE_THRESHOLD }}" >> $GITHUB_STEP_SUMMARY
          fi